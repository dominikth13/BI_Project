{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "788d7052",
   "metadata": {},
   "source": [
    "# Data Modeling\n",
    "Zuerst vorbereitete Daten einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4d04d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "\n",
    "cleaned_data = pd.read_csv('cleaned_data.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e83d1f6",
   "metadata": {},
   "source": [
    "## Dominik\n",
    "- zuerst Tests mit Entscheidungsbäumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3533e54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dec_tree = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0e678d3",
   "metadata": {},
   "source": [
    "- nun wählen wir aus, für welches Subset der `cleaned_data` wir den Baum befüllen wollen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "590d4916",
   "metadata": {},
   "source": [
    "- zuerst wählen wir die folgenden Daten aus: `DATE.OCC`, `TIME.OCC`, `AREA.NAME` -> `Crm.Cd`\n",
    "- dies ist die gröbste Aufteilung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dd612a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11571617764397174"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['UNIX.TIMESTAMP','AREA.NAME']\n",
    "target = 'Crm.Cd'\n",
    "selection = features + [target]\n",
    "\n",
    "predict_data = cleaned_data[selection].copy()\n",
    "predict_data_encoded = pd.get_dummies(predict_data, columns=['AREA.NAME'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(predict_data_encoded.drop(target, axis=1), predict_data_encoded[target], test_size=0.2, random_state=42)\n",
    "dec_tree.fit(X_train,y_train)\n",
    "\n",
    "print(\"Model Accuracy:\")\n",
    "dec_tree.score(X_test,y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab91dd8e",
   "metadata": {},
   "source": [
    "- ist scheiße, eine Andere Möglichkeit wäre, die Verbrechen grober zu Klassifizieren, hier fragen wir ChatGPT in Transformation\n",
    "- Ergebnis: neue Attribut CRIME_VIOLENT kann predicted werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ddccda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6686367412745026"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['UNIX.TIMESTAMP','AREA.NAME']\n",
    "target = 'CRIME_VIOLENT'\n",
    "selection = features + [target]\n",
    "\n",
    "predict_data = cleaned_data[selection].copy()\n",
    "predict_data_encoded = pd.get_dummies(predict_data, columns=['AREA.NAME'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(predict_data_encoded.drop(target, axis=1), predict_data_encoded[target], test_size=0.2, random_state=42)\n",
    "dec_tree.fit(X_train,y_train)\n",
    "\n",
    "print(\"Model Accuracy:\")\n",
    "dec_tree.score(X_test,y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a578d93d",
   "metadata": {},
   "source": [
    "- deutlich besser, aber natürlich wären mehr Kategorien schon schön\n",
    "- nun mit `CRIME_CAT`\n",
    "- es gibt 17 verschiedene Kategorien\n",
    "- mal schauen, wie dies performt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deee9dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.33242438450591844"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['UNIX.TIMESTAMP','AREA.NAME']\n",
    "target = 'CRIME_CAT'\n",
    "selection = features + [target]\n",
    "\n",
    "predict_data = cleaned_data[selection].copy()\n",
    "predict_data_encoded = pd.get_dummies(predict_data, columns=['AREA.NAME'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(predict_data_encoded.drop(target, axis=1), predict_data_encoded[target], test_size=0.2, random_state=42)\n",
    "dec_tree.fit(X_train,y_train)\n",
    "\n",
    "print(\"Model Accuracy:\")\n",
    "dec_tree.score(X_test,y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f38089fb",
   "metadata": {},
   "source": [
    "- nun testen wir zusätzlich mit Koordinaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "135dfe23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.41221446256858246"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['UNIX.TIMESTAMP','AREA.NAME','LAT','LONG']\n",
    "target = 'CRIME_CAT'\n",
    "selection = features + [target]\n",
    "\n",
    "predict_data = cleaned_data[selection].copy()\n",
    "# Filtern von null (muss nur für eines gemacht werden, da Koordinaten nie alleine vorkommen)\n",
    "predict_data = predict_data[predict_data['LONG'].notnull()]\n",
    "predict_data_encoded = pd.get_dummies(predict_data, columns=['AREA.NAME'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(predict_data_encoded.drop(target, axis=1), predict_data_encoded[target], test_size=0.2, random_state=42)\n",
    "dec_tree.fit(X_train,y_train)\n",
    "\n",
    "print(\"Model Accuracy:\")\n",
    "dec_tree.score(X_test,y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bd2a05f",
   "metadata": {},
   "source": [
    "- Genauigkeit wird nur leicht erhöht\n",
    "- nun versuchen wir die Datums und Zeitinformationen besser zu verwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9682512f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.4103883391325091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\I586521\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.03152941950661764\n"
     ]
    }
   ],
   "source": [
    "features = ['DATE.OCC_day','DATE.OCC_month','TIME.OCC_hour','AREA.NAME','LAT','LONG']\n",
    "target = 'CRIME_CAT'\n",
    "selection = features + [target]\n",
    "\n",
    "predict_data = cleaned_data[selection].copy()\n",
    "# Filtern von null (muss nur für eines gemacht werden, da Koordinaten nie alleine vorkommen)\n",
    "predict_data = predict_data[predict_data['LONG'].notnull()]\n",
    "predict_data_encoded = pd.get_dummies(predict_data, columns=['AREA.NAME'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(predict_data_encoded.drop(target, axis=1), predict_data_encoded[target], test_size=0.2, random_state=42)\n",
    "dec_tree.fit(X_train,y_train)\n",
    "\n",
    "print(f'Model Accuracy: {dec_tree.score(X_test,y_test)}')\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# log_regr = LogisticRegression(class_weight='balanced')\n",
    "# log_regr.fit(X_train,y_train)\n",
    "\n",
    "# print(f'Model Accuracy: {log_regr.score(X_test,y_test)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3cd0675",
   "metadata": {},
   "source": [
    "- Logistische Regression scheint überhaupt nicht zu funktionieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bc2597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import svm\n",
    "# svm_class = svm.SVC(kernel='linear')\n",
    "\n",
    "# features = ['UNIX.TIMESTAMP','AREA.NAME','LAT','LONG']\n",
    "# target = 'CRIME_CAT'\n",
    "# selection = features + [target]\n",
    "\n",
    "# predict_data = cleaned_data[selection].copy()\n",
    "# # Filtern von null (muss nur für eines gemacht werden, da Koordinaten nie alleine vorkommen)\n",
    "# predict_data = predict_data[predict_data['LONG'].notnull()]\n",
    "# predict_data_encoded = pd.get_dummies(predict_data, columns=['AREA.NAME'])\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(predict_data_encoded.drop(target, axis=1), predict_data_encoded[target], test_size=0.2, random_state=42)\n",
    "# svm_class.fit(X_train,y_train)\n",
    "\n",
    "# print(\"Model Accuracy:\")\n",
    "# svm_class.score(X_test,y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c247c0d2",
   "metadata": {},
   "source": [
    "- SVM zu lange Laufzeit, um getestet zu werden (nach 30min abgebrochen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5ffb5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.41445598795745625"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Build a Gaussian Classifier\n",
    "nb_class = GaussianNB()\n",
    "\n",
    "features = ['UNIX.TIMESTAMP','AREA.NAME','LAT','LONG']\n",
    "target = 'CRIME_CAT'\n",
    "selection = features + [target]\n",
    "\n",
    "predict_data = cleaned_data[selection].copy()\n",
    "# Filtern von null (muss nur für eines gemacht werden, da Koordinaten nie alleine vorkommen)\n",
    "predict_data = predict_data[predict_data['LONG'].notnull()]\n",
    "predict_data_encoded = pd.get_dummies(predict_data, columns=['AREA.NAME'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(predict_data_encoded.drop(target, axis=1), predict_data_encoded[target], test_size=0.2, random_state=42)\n",
    "nb_class.fit(X_train,y_train)\n",
    "\n",
    "print(\"Model Accuracy:\")\n",
    "nb_class.score(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
